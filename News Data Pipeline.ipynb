{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67561b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This library is used to download contents of a webpage or get data from API\n",
    "import requests\n",
    "\n",
    "#Used along with requests to scrape and extract data from web pages\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#To interact with SQLite databases\n",
    "import sqlite3\n",
    "\n",
    "#To handle and analyze structured data\n",
    "import pandas as pd\n",
    "\n",
    "#To work with date and time objects\n",
    "from datetime import datetime\n",
    "\n",
    "#To access time-related functions\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b0691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a database and table if it doesn’t exist\n",
    "def init_db():\n",
    "    conn = sqlite3.connect('news_articles.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS articles(\n",
    "                        article_id TEXT PRIMARY KEY,\n",
    "                        url TEXT,\n",
    "                        title TEXT,\n",
    "                        publication_ts TEXT,\n",
    "                        source TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a8d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scrape articles from the homepage of [Skift](https://skift.com/) and return them as a list of dictionaries\n",
    "def scrape_skift():\n",
    "    url = \"https://skift.com/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    for tag in soup.select('article'):\n",
    "        a_tag = tag.find('a', href=True)\n",
    "        title_tag = tag.find(['h2', 'h3'])\n",
    "        date_tag = tag.find('time')\n",
    "        \n",
    "        if a_tag and title_tag and date_tag:\n",
    "            article_url = a_tag['href']\n",
    "            title = title_tag.text.strip()\n",
    "            date = date_tag.get('datetime', datetime.now().isoformat())\n",
    "            article_id = article_url  # Use URL as unique ID\n",
    "            \n",
    "            articles.append({\n",
    "                'article_id': article_id,\n",
    "                'url': article_url,\n",
    "                'title': title,\n",
    "                'publication_ts': date,\n",
    "                'source': 'Skift'\n",
    "            })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a379f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract recent articles from [PhocusWire](https://www.phocuswire.com/) and return them in structured format.\n",
    "def scrape_phocuswire():\n",
    "    url = \"https://www.phocuswire.com/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    for tag in soup.select('.article-list .article-item'):\n",
    "        a_tag = tag.find('a', href=True)\n",
    "        title_tag = tag.find('h2')\n",
    "        date_tag = tag.find('time')\n",
    "\n",
    "        if a_tag and title_tag and date_tag:\n",
    "            article_url = \"https://www.phocuswire.com\" + a_tag['href']\n",
    "            title = title_tag.text.strip()\n",
    "            date = date_tag.get('datetime', datetime.now().isoformat())\n",
    "            article_id = article_url\n",
    "\n",
    "            articles.append({\n",
    "                'article_id': article_id,\n",
    "                'url': article_url,\n",
    "                'title': title,\n",
    "                'publication_ts': date,\n",
    "                'source': 'PhocusWire'\n",
    "            })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4300d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store new articles\n",
    "def store_articles(articles):\n",
    "    conn = sqlite3.connect('news_articles.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO articles (article_id, url, title, publication_ts, source)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                article['article_id'],\n",
    "                article['url'],\n",
    "                article['title'],\n",
    "                article['publication_ts'],\n",
    "                article['source']\n",
    "            ))\n",
    "        except sqlite3.IntegrityError:\n",
    "            # Duplicate article_id; skip\n",
    "            continue\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee9693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Top 5 Latest Articles\n",
    "def get_top_articles(n=5):\n",
    "    conn = sqlite3.connect('news_articles.db')\n",
    "    df = pd.read_sql_query('''\n",
    "        SELECT * FROM articles ORDER BY publication_ts DESC LIMIT ?\n",
    "    ''', conn, params=(n,))\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c215d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline execution\n",
    "def run_pipeline():\n",
    "    print(\"Initializing DB...\")\n",
    "    init_db()\n",
    "    \n",
    "    print(\"Scraping Skift...\")\n",
    "    skift_articles = scrape_skift()\n",
    "    \n",
    "    print(\"Scraping PhocusWire...\")\n",
    "    phocus_articles = scrape_phocuswire()\n",
    "\n",
    "    print(\"Storing articles...\")\n",
    "    all_articles = skift_articles + phocus_articles\n",
    "    store_articles(all_articles)\n",
    "\n",
    "    print(\"Top 5 Latest Articles:\")\n",
    "    latest = get_top_articles()\n",
    "    print(latest[['title', 'publication_ts', 'source']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4535ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DB...\n",
      "Scraping Skift...\n",
      "Scraping PhocusWire...\n",
      "Storing articles...\n",
      "Top 5 Latest Articles:\n",
      "                                               title  \\\n",
      "0  Air India Crash: A Day Later, What We Know So Far   \n",
      "1  Air India Crash Deepens Traveler Anxiety Aroun...   \n",
      "2  Tripadvisor Is Turning 25. Here’s What Its CEO...   \n",
      "3  The Private Sector’s Role in Saudi’s Travel Am...   \n",
      "4  India’s Expanding Airline Fleets: How Boeing a...   \n",
      "\n",
      "              publication_ts source  \n",
      "0  2025-06-13T10:03:47-04:00  Skift  \n",
      "1  2025-06-13T09:57:54-04:00  Skift  \n",
      "2  2025-06-13T09:51:44-04:00  Skift  \n",
      "3  2025-06-13T09:27:05-04:00  Skift  \n",
      "4  2025-06-13T09:10:53-04:00  Skift  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0f880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
